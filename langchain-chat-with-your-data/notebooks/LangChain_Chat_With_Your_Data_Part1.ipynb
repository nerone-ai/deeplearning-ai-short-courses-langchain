{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b744a4",
   "metadata": {},
   "source": [
    "# LangChain - Chat With Your Data: Part 1\n",
    "\n",
    "Tutorial on document ingestion using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa13d1c",
   "metadata": {},
   "source": [
    "### Background: Why Retrieval-Augmented Generation (RAG)?\n",
    "Large Language Models (LLMs) cannot access information outside their training data. Retrieval-Augmented Generation (RAG) enables LLMs to fetch and use external documents (e.g., PDFs, web pages, notes) to generate more accurate and context-aware responses.\n",
    "\n",
    "### Key Concept: RAG Pipeline\n",
    "1. **Document Loading** – convert various data formats into `Document` objects.\n",
    "2. **Splitting** – chunk the documents into manageable parts.\n",
    "3. **Embedding & Indexing** – embed chunks and store them in a vector database.\n",
    "4. **Retrieval** – pull relevant chunks based on query.\n",
    "5. **Generation** – combine query and retrieved context to generate output.\n",
    "\n",
    "This notebook focuses on **Step 1: Document Loading**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da5023",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5742be",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai\n",
    "\n",
    "sys.path.append('../..')\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c1157",
   "metadata": {},
   "source": [
    "### Example 1: Loading PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a545065",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install pypdf\n",
    "\n",
    "#!pip install yt_dlp\n",
    "#!pip install pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be8ecbb",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 22\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is ju st spend a little time going over the logistics \n",
      "of the class, and then we'll start to  talk a bit about machine learning.  \n",
      "By way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \n",
      "I personally work in machine learning, and I' ve worked on it for about 15 years now, and \n",
      "I actually think that machine learning i\n",
      "{'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Print the number of pages and the first page's content\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(pages[0].page_content[:500])\n",
    "\n",
    "# Print the metadata of the first page, there is a source and page number\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697eea9",
   "metadata": {},
   "source": [
    "### Example 2: Loading and Transcribing YouTube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66da23e2",
   "metadata": {
    "height": 268
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing part 1!\n",
      "Welcome to CS229 Machine Learning. Uh, some of you know that this is a class that's taught at Stanford for a long time. And this is often the class that, um, I most look forward to teaching each year because this is where we've helped, I think, several generations of Stanford students become experts in machine learning, got- built many of their products and services and startups that I'm sure, many of you or probably all of you are using, uh, uh, today. Um, so what I want to do today was spend s\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader#, FileSystemBlobLoader\n",
    "from langchain.document_loaders.blob_loaders import FileSystemBlobLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "save_dir = \"docs/youtube/\"\n",
    "\n",
    "loader = GenericLoader(\n",
    "    ##YoutubeAudioLoader([url],save_dir),  # fetch from youtube\n",
    "    FileSystemBlobLoader(save_dir, glob=\"*.m4a\"), # load from local\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428e1c5",
   "metadata": {},
   "source": [
    "### Example 4:  Loading Content from Web Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68004c4",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "handbook/titles-for-programmers.md at master · basecamp/handbook · GitHub\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Navigation Menu\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Sign in\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Product\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Copilot\n",
      "        Write better code with AI\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Advanced Security\n",
      "        Find and fix vulnerabilities\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "###  Loading Web Pages\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/titles-for-programmers.md\")\n",
    "docs = loader.load()\n",
    "# print the first page content, notice this needs pre-processing\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69e6a3",
   "metadata": {},
   "source": [
    "### Example 5: Loading Notion Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb97413",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Blendle's Employee Handbook\n",
      "\n",
      "This is a living document with everything we've learned working with people while running a startup. And, of course, we continue to learn. Therefore it's a document that\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "notion_path = \"docs/Notion_DB\"\n",
    "loader = NotionDirectoryLoader(notion_path)\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7440a",
   "metadata": {},
   "source": [
    "### Summary\n",
    "You've learned how to load documents from:\n",
    "- **PDFs** (via `PyPDFLoader`)\n",
    "- **YouTube** (via Whisper + `YoutubeAudioLoader`)\n",
    "- **Web Pages** (via `WebBaseLoader`)\n",
    "- **Notion** (via `NotionDirectoryLoader`)\n",
    "\n",
    "All loaders return `Document` objects with `.page_content` and `.metadata`.\n",
    "\n",
    "**Next Step:** Split documents into chunks for semantic search and efficient retrieval in RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
