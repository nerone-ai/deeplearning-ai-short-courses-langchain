{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd0934e",
   "metadata": {},
   "source": [
    "# LangChain - Chat With Your Data: Part 2\n",
    "\n",
    "## ðŸ“š Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586d0e2",
   "metadata": {},
   "source": [
    "Splitting documents into smaller chunks is critical before storing them in vector stores.\n",
    "This ensures that, during retrieval, the system fetches semantically meaningful and complete information.\n",
    "\n",
    "**Challenges:**\n",
    "- Splitting on a fixed number of characters may break sentences or logic.\n",
    "- Need to preserve semantic context.\n",
    "- Must retain or enrich metadata.\n",
    "\n",
    "LangChain offers a rich suite of text splitters with various strategies, from character-based to token-based to header-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b58ad",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b0c534",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Load environment variables, including OPENAI_API_KEY\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91184c",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter vs RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bdd3e1",
   "metadata": {
    "height": 285
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijklmnopqrstuvwxyz']\n"
     ]
    }
   ],
   "source": [
    "# The two most common text splitters are RecursiveCharacterTextSplitter and CharacterTextSplitter.\n",
    "# However, LangChain offers a rich suite of text splitters with various strategies, \n",
    "# from character-based to token-based to header-aware.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# Define chunk size and overlap \n",
    "# Overlap is the number of tokens to overlap between chunks (like a sliding window)\n",
    "chunk_size = 26\n",
    "chunk_overlap = 4\n",
    "\n",
    "# Initialize splitters\n",
    "r_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print(r_splitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4fdeb",
   "metadata": {},
   "source": [
    "Notice that the RecursiveCharacterTextSplitter splits the text into chunks of 26 characters, with an overlap of 4 characters between chunks. So it didn't split text1 as it fits into 1 chunk. \n",
    "However it splits a longer text (text2) and also text3 with spaces into multiple chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3914cb72",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']\n"
     ]
    }
   ],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
    "print(r_splitter.split_text(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cfac57",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n"
     ]
    }
   ],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "print(r_splitter.split_text(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d9643",
   "metadata": {},
   "source": [
    "Notice that the CharacterTextSplitter by default uses a '\\n' as separator, so it doesn't split text 3, except if the whitespace separator is explicitly specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd052c55",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a b c d e f g h i j k l m n o p q r s t u v w x y z']\n"
     ]
    }
   ],
   "source": [
    "print(c_splitter.split_text(text3))  # Default uses '\\n' as separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b86dbf8",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n"
     ]
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator=' ')\n",
    "print(c_splitter.split_text(text3)) # Uses ' ' as separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f4b54",
   "metadata": {},
   "source": [
    "### Recursive Splitting with Paragraphs and Regex\n",
    "\n",
    "Example of using RecursiveCharacterTextSplitter with a regex to split on text paragraphs.\n",
    "The separators are defined as `\\n\\n`, `\\n`, ` ` (space), and `\"\"` (empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f27eb60",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "some_text = (\n",
    "    \"When writing documents, writers will use document structure to group content. \"\n",
    "    \"This can convey to the reader, which ideas are related. For example, closely related ideas \"\n",
    "    \"are in sentences. Similar ideas are in paragraphs. Paragraphs form a document.\\n\\n\"\n",
    "    \"Paragraphs are often delimited with a carriage return or two carriage returns. \"\n",
    "    \"Carriage returns are the 'backslash n' you see embedded in this string. \"\n",
    "    \"Sentences have a period at the end, \"\n",
    "    \"but also, have a space. and words are separated by space.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546b9278",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which ideas are related. For example, closely related ideas are in sentences. Similar ideas are in paragraphs. Paragraphs form a document.\\n\\nParagraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the 'backslash n' you see embedded in this string. Sentences have a period at the end, but also, have\", 'a space. and words are separated by space.']\n",
      "['When writing documents, writers will use document structure to group content. This can convey to the reader, which ideas are related. For example, closely related ideas are in sentences. Similar ideas are in paragraphs. Paragraphs form a document.', \"Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the 'backslash n' you see embedded in this string. Sentences have a period at the end, but also, have a space. and words are separated by space.\"]\n"
     ]
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=450, chunk_overlap=0, separator=' ')\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450, chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "print(c_splitter.split_text(some_text))\n",
    "print(r_splitter.split_text(some_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1b3f7",
   "metadata": {},
   "source": [
    "The RecursiveCharacterTextSplitter is a bit more advanced, but it can be a bit more complex to use. Let's reduce the chunk size a bit and add a period to our separators. Notice the need for a lookbehind in the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894164c7",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When writing documents, writers will use document structure to group content. This can convey to the reader, which ideas are related. For example, clo', 'sely related ideas are in sentences. Similar ideas are in paragraphs. Paragraphs form a document.', \"Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the 'backslash n' you see embedded in this string\", '. Sentences have a period at the end, but also, have a space. and words are separated by space.']\n"
     ]
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "print(r_splitter.split_text(some_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829b29ee",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When writing documents, writers will use document structure to group content. This can convey to the reader, which ideas are related.', 'For example, closely related ideas are in sentences. Similar ideas are in paragraphs. Paragraphs form a document.', 'Paragraphs are often delimited with a carriage return or two carriage returns.', \"Carriage returns are the 'backslash n' you see embedded in this string. Sentences have a period at the end, but also, have a space.\", 'and words are separated by space.']\n"
     ]
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\\\. )\", \" \", \"\"]\n",
    ")\n",
    "print(r_splitter.split_text(some_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79331c6",
   "metadata": {},
   "source": [
    "### Token-based Splitting\n",
    "\n",
    "TokenTextSplitter splits text based on token count (e.g., GPT-style tokens).\n",
    "This is useful when preparing chunks that must respect the LLM's context window (measured in tokens, not characters).\n",
    "Approximately, 1 token â‰ˆ 4 characters of English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b93a2e",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output with chunk_size=1: ['foo', ' bar', ' b', 'az', 'zy', 'foo']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Example 1: Basic tokenization (chunk_size=1 for individual tokens)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "text1 = \"foo bar bazzyfoo\"\n",
    "tokens = text_splitter.split_text(text1)\n",
    "\n",
    "print(\"Tokenized output with chunk_size=1:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1100658b",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First split document: MachineLearning-Lecture01  \n",
      "\n",
      "Metadata retained: {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Larger chunk size - first load the document\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# This will group up to 10 tokens together per chunk.\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)  # `pages` assumed loaded from previous PDF example\n",
    "\n",
    "print(\"First split document:\", docs[0].page_content)\n",
    "print(\"Metadata retained:\", docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73807b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Token-based splitting aligns better with transformer model constraints.\n",
    "- Useful when targeting specific models like GPT-3.5, GPT-4, etc., which have max token limits.\n",
    "\n",
    "### Context-Aware Splitting\n",
    "\n",
    "Context-aware splitting is a more advanced method that groups text based on semantic similarity rather than fixed token boundaries.\n",
    "\n",
    "Chunking aims to keep text with common context together. A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting. MarkdownHeaderTextSplitter can be used to preserve header metadata in the chunks, as show below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0e99bc",
   "metadata": {
    "height": 608
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 content:\n",
      "Hi this is Jim  \n",
      "Hi this is Joe\n",
      "\n",
      "Metadata: {'Header 1': 'Title', 'Header 2': 'Chapter 1'}\n",
      "----------------------------------------\n",
      "Chunk 2 content:\n",
      "Hi this is Lance\n",
      "\n",
      "Metadata: {'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'}\n",
      "----------------------------------------\n",
      "Chunk 3 content:\n",
      "Hi this is Molly\n",
      "\n",
      "Metadata: {'Header 1': 'Title', 'Header 2': 'Chapter 2'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Some documents (especially wikis, technical notes, Notion exports) are structured with markdown headers.\n",
    "# This splitter extracts and attaches headers (H1, H2, H3, etc.) as metadata for each chunk.\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"\"\"# Title\n",
    "\n",
    "## Chapter 1\n",
    "\n",
    "Hi this is Jim\n",
    "\n",
    "Hi this is Joe\n",
    "\n",
    "### Section\n",
    "\n",
    "Hi this is Lance\n",
    "\n",
    "## Chapter 2\n",
    "\n",
    "Hi this is Molly\"\"\"\n",
    "\n",
    "# Define which markdown headers to capture and their metadata labels\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "md_chunks = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Examine output: content + metadata\n",
    "for idx, chunk in enumerate(md_chunks[:3]):\n",
    "    print(f\"Chunk {idx + 1} content:\\n{chunk.page_content}\\n\")\n",
    "    print(f\"Metadata: {chunk.metadata}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd21f0d",
   "metadata": {
    "height": 387
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example split with header-based metadata:\n",
      "\n",
      "\n",
      "Content: This is a living document with everything we've learned working with people while running a startup. And, of course, we continue to learn. Therefore it's a document that will continue to change.  \n",
      "**Everything related to working at Blendle and the people of Blendle, made public.**  \n",
      "These are the lessons from three years of working with the people of Blendle. It contains everything from [how our leaders lead](https://www.notion.so/ecfb7e647136468a9a0a32f1771a8f52?pvs=21) to [how we increase salaries](https://www.notion.so/Salary-Review-e11b6161c6d34f5c9568bb3e83ed96b6?pvs=21), from [how we hire](https://www.notion.so/Hiring-451bbcfe8d9b49438c0633326bb7af0a?pvs=21) and [fire](https://www.notion.so/Firing-5567687a2000496b8412e53cd58eed9d?pvs=21) to [how we think people should give each other feedback](https://www.notion.so/Our-Feedback-Process-eb64f1de796b4350aeab3bc068e3801f?pvs=21) â€” and much more.  \n",
      "We've made this document public because we want to learn from you. We're very much interested in your feedback (including weeding out typo's and Dunglish ;)). Email us at hr@blendle.com. If you're starting your own company or if you're curious as to how we do things at Blendle, we hope that our employee handbook inspires you.  \n",
      "If you want to work at Blendle you can check our [job ads here](https://blendle.homerun.co/). If you want to be kept in the loop about Blendle, you can sign up for [our behind the scenes newsletter](https://blendle.homerun.co/yes-keep-me-posted/tr/apply?token=8092d4128c306003d97dd3821bad06f2).\n",
      "\n",
      "\n",
      "Metadata: {'Header 1': \"Blendle's Employee Handbook\"}\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§ª Try this on a real Notion Markdown export (previously loaded Notion DB)\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "notion_docs = loader.load()\n",
    "\n",
    "# Combine content from all documents for header splitting\n",
    "full_markdown_text = \" \".join([doc.page_content for doc in notion_docs])\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "])\n",
    "\n",
    "md_splits = markdown_splitter.split_text(full_markdown_text)\n",
    "\n",
    "# Print first structured chunk\n",
    "print(\"Example split with header-based metadata:\")\n",
    "print(\"\\n\")\n",
    "print(\"Content:\", md_splits[0].page_content)\n",
    "print(\"\\n\")\n",
    "print(\"Metadata:\", md_splits[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93077f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- This approach is context-aware and ensures chunks preserve logical section boundaries.\n",
    "- Very effective when used with structured technical documents or user manuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5e7b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- LangChain provides various **splitting techniques**: character, recursive, token-based, and header-aware.\n",
    "- RecursiveCharacterTextSplitter is generally most robust.\n",
    "- Token-based splitting is helpful for LLM context sizing.\n",
    "- MarkdownHeaderTextSplitter allows semantic, structure-preserving splits.\n",
    "\n",
    "**Next Step:** Use vector stores and embeddings to store and retrieve these chunks for answering questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
